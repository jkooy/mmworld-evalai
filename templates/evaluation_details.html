<p>The evaluation phase involves checking submissions with strict matching based on the criteria: a, b, c, or d, without any additional words. Additionally, we will utilize the GPT-4 API to map participants' answers with the candidate answers for comprehensive assessment. This challenge includes the evaluation of both proprietary and open-source MLLMs, aiming to identify and improve the current limitations observed in the models' performance on MMWorld.</p>
<p>The evaluation outputs use accuracy for each question-answer pair (QA) being the primary evaluation metric:</p>
<p>1. **Accuracy for Each QA**: The primary metric, measuring the percentage of correctly answered question-answer pairs, based on exact matching criteria.</p>
<p>Note: Please click the "Submit" tab (available only after logging in and participating in the challenge) for submission guidelines.</p>