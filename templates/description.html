<p>Multimodal Language Models (MLLMs) demonstrate the emerging abilities of "world models"---interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities. To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding. 
    <br>
<p>Based on the MMWorld dataset, we present the MMWorld Challenge. The goal of the challenge is to evaluate MLLMs on their ability to perform multi-faceted reasoning, including explanation, counterfactual thinking, and future prediction, across various disciplines. MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about entire videos and a synthetic dataset to analyze MLLMs within a single modality of perception. It includes 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions.</p>
<p>The evaluation phase involves checking submissions with strict matching based on the criteria: a, b, c, or d, without any additional words. Additionally, we will utilize the GPT-4 API to map participants' answers with the candidate answers for comprehensive assessment. This challenge includes the evaluation of both proprietary and open-source MLLMs, aiming to identify and improve the current limitations observed in the models' performance on MMWorld.</p>